ad# 2026-01-16 - failure log

**day:** thursday

## failures

**f1: pipeline infrastructure not matching the architecture described in the proposal**

coded a modular system: Module A coordinates integration, Module B handles per-assay processing, Module C extracts TF activity, Module D builds 3D chromatin predictors. implemented it in Python with class hierarchies, JSON config files, documented interfaces. reality: when i actually ran the end-to-end pipeline, Module D crashed when contact matrix normalization changed mid-stream. outputs weren't persisted consistently. chaining modules together revealed interface mismatches i hadn't caught during coding because each module worked in isolation.

problem: i built bottom-up without verifying modules could actually talk to each other. coded Module D three separate times because the interface assumptions changed. the integration layer i wrote assumed outputs that Module C didn't actually produce. now debugging the actual code reveals gaps i missed during implementation.

**f2: data cohort assembly broke during coding**

i coded the data loader expecting six standardized datasets. wrote parsers for .cool, .hic, and .h5 formats. got partial access to only three. coded format converters to stitch everything into .h5mu containers. then ran the code: column names didn't match across datasets, batch effect correction i wrote assumed normalized inputs but inputs were raw, Hi-C filtering thresholds varied between replicates in ways i didn't account for. the hum0257.v2 organoid cohort i parsed has massive files (159-493 GB) and my code crashed on OOM because i didn't implement chunked I/O.

i spent two weeks rewriting the loaders multiple times. every time i coded a fix for one dataset, another dataset exposed a different problem. wrote five different versions of the metadata alignment code. can't even run my own pipeline: do we have N or N-1 samples? the code doesn't know because the versioning wasn't something i could code around.

**f3: coded a GNN that doesn't validate**

i implemented a Graph Attention Network (GAT) in PyTorch for enhancer-to-gene link prediction. trained it on HiChIP loop labels. wrote the data pipeline, the model forward pass, the loss function, the training loop. ran inference on 100 enhancer-promoter pairs. got F1 = 0.52. random baseline is 0.5. my code produced essentially zero signal.

debugged for three days: trained on the wrong label dimension? no. features aren't normalized? no. model is learning distance penalties instead of actual links? probably. wrote ablation code to test distance-only model—it also gets F1 = 0.52. the code is correct; the features or labels are insufficient. i can't tell from the model i coded whether it's learning TAD structure or just memorizing geometry.

**f4: started coding formal verification but ran into specification gaps**

i coded a stormpy integration and started building the CIR (Circuit Intermediate Representation). wrote the node/edge classes, the state transition logic, tried to instantiate it with actual chromatin data. immediately realized: what exactly am i trying to verify? the code i wrote assumes certain safety properties but never defines them formally. i have a stochastic model in Python but stormpy doesn't know what to check. this was supposed to be a differentiator but it's currently half-written code with no specification backing it.

## root causes

**rc1 (f1):** i coded modules in isolation. Module tests pass independently but integration tests fail. should have written the integration layer first (a thin orchestrator), then built each module to match that interface contract. instead i coded each module, then tried to bolt them together.

**rc2 (f2):** i wrote a data loader without understanding the actual data. coded format converters for theoretical datasets, then discovered real data has edge cases my code doesn't handle. should have manually inspected a few samples first, then coded the loaders.

**rc3 (f3):** i jumped to GAT implementation without establishing what a working baseline looks like. should have coded a distance-only model first, validated that it beats random, then added features incrementally. built complexity first, validation last.

**rc4 (f4):** i started coding the CIR without a spec. should have written the specification (what nodes/edges exist, what transitions are allowed, what properties to verify) before writing a single line of code.

## lessons

- integration layer should come first in the code. code one module, then write the integration that calls it, then write the next module to match. integrating existing code is hard; architecturing around integration is easier.
- inspect raw data before writing loaders. five minutes of manual inspection saves five days of debugging format edge cases.
- code a dumb baseline first. get it working. only then add complexity. if fancy code doesn't beat simple code, the problem isn't the model.
- write specs before code. formal verification, data schemas, interfaces—these need to be written first. they constrain the code that follows.
- test the whole pipeline, not just the parts. unit tests pass but integration fails is a sign the integration layer needs to be designed first.

## resolution & next steps

action items:
- [ ] pick one complete example (PANC-1 cell type) and get a full Module A→B→C→D pipeline working end-to-end, even if limited
- [ ] define Module interfaces explicitly: what does A output? what does B expect? document in code
- [ ] audit data cohort: determine actual N, check formats, create unified metadata table, flag missing pieces
- [ ] establish GNN baseline: test distance-only, TAD-matching only, then feature ablations to understand what drives predictions
- [ ] write CIR spec (formal schema): what is a node? what is an edge? what are the parameters? only then build verification logic

longer term:
- [ ] don't write papers about code that doesn't exist
- [ ] build incrementally, validate frequently, publish one working module before the full system
- [ ] pair with someone who's done this before (data heavy + formal methods + graph models is three hard problems)

## reflection

the project is interesting but i was too ambitious on the timeline and too confident in the design. proposed a whole pipeline without actually building it. that's not engineering, that's fiction. 

the gap between "what i think the system should do" and "what i can demonstrate it does" is huge. that gap is where failure lives. next time the proposal reflects what's already working, not what i hope will work.

real talk: this could still be good work, but it needs to be rebuilt bottom-up. modules should work independently first. data should be clean and versioned before modeling. baselines should be established before fancy methods. and the formal stuff should be spec'd before it's promised.

going to spend the next week building Module A + B for one cell type and just getting that right. no more abstractions until i can show working code.