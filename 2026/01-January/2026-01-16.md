# 2026-01-16 - failure log

**day:** thursday

## failures

**f1: pipeline infrastructure not matching the architecture described in the proposal**

designed a beautiful modular system: Module A coordinates integration, Module B handles per-assay processing, Module C extracts TF activity, Module D builds 3D chromatin predictors, etc. wrote it up cleanly. reality: have partially working scripts scattered across jupyter notebooks, no versioned artifact outputs, no unified object container structure. Module D graph construction started three times, each time abandoned when the contact matrix normalization approach changed. when it came time to actually run end-to-end, couldn't chain modules together because interfaces weren't defined.

problem: spec was written top-down (what should exist) without building bottom-up (what actually works first). spent time writing the proposal to funders instead of writing the code to verify it was possible. now the gap between promise and implementation is obvious and embarrassing.

**f2: data cohort assembly is a nightmare of access, formats, and versioning**

proposal lists six different datasets with specific sample counts and assay types. reality: got partial access to three. some data is in .cool format, some in raw hic reads, some not yet processed. the organoid realism cohort (hum0257.v2) has massive Hi-C files (159-493 GB) but documentation is sparse and unclear whether samples are actually PDAC or just adenocarcinoma-adjacent. hi-c replicates have inconsistent filtering thresholds. tried to stitch everything into a unified multi-modal container (.h5mu) but formats don't align, column names don't match, batch effects are unmapped.

lost two weeks just getting sample metadata straight. the "versioned feature stores" mentioned in abstract don't exist yet. can't even answer: do we have N or N-1 samples ready for Module C?

**f3: graph neural network baseline is slow and the link prediction isn't validating**

said we'd use GAT (Graph Attention Networks) + graph transformers for enhancer-to-gene link prediction, trained on HiChIP loop labels. built the model. ran it on a subset of 100 enhancer-promoter pairs. F1 score was 0.52. with random baseline at 0.5, basically no signal. either the features are insufficient, the HiChIP labels are too sparse, or the model architecture is wrong. haven't debugged which.

what's worse: no interpretation. can't tell if the model learned TAD structure or just memorized distance penalties. spent 3 days on training curves instead of understanding what the model was actually capturing.

**f4: formal verification setup is theoretical but not integrated**

proposal mentions "formal safety checks via probabilistic model checking (stormpy)." sounds rigorous. reality: haven't actually built a stochastic model that stormpy can verify. the CIR (Circuit Intermediate Representation) is described but not specified. can't formally verify something if you haven't formally specified what you're verifying. this was supposed to be a differentiator but it's currently just a citation.

## root causes

**rc1 (f1):** spec-first development. wrote the architecture to impress reviewers before proving modules could talk to each other. should have built one module end-to-end, integrated it with the next, and only then written about the full system. premature abstraction.

**rc2 (f2):** data acquisition underestimated. assumed datasets would be standardized and accessible. didn't plan for: embargoes, format fragmentation, incomplete metadata, or the fact that "matched multi-omic series" in papers often means "matched *conceptually* but collected differently." data wrangling is the real bottleneck, not the analysis.

**rc3 (f3):** no baseline expectations. jumped to fancy models (graph transformers) without establishing what simple methods achieve. no sanity check: does proximity alone beat the GNN? does exact TAD matching beat contact prediction? built complexity first, diagnostic last.

**rc4 (f4):** formal methods treated as a marketing feature instead of a requirement. should have started with: "what properties do we need to verify?" then built CIR to satisfy that. instead built a cool graph and asked "can we verify it?" backward.

## lessons

- architecture on paper ≠ architecture in code. build small, integrate, then abstract.
- data cleaning is not a one-time step. it's a continuous burden. budget 40% of the project for data pipeline work, not 10%.
- always have a dumb baseline. if your fancy model doesn't beat simple rules, the problem is your feature engineering, not your model.
- formal verification only works if you formally specify what you're verifying first. specifications aren't optional.
- "versioned artifacts" and "open-access pipeline" are promises. they need to be built before the proposal is submitted, not after.

## resolution & next steps

action items:
- [ ] pick one complete example (PANC-1 cell type) and get a full Module A→B→C→D pipeline working end-to-end, even if limited
- [ ] define Module interfaces explicitly: what does A output? what does B expect? document in code
- [ ] audit data cohort: determine actual N, check formats, create unified metadata table, flag missing pieces
- [ ] establish GNN baseline: test distance-only, TAD-matching only, then feature ablations to understand what drives predictions
- [ ] write CIR spec (formal schema): what is a node? what is an edge? what are the parameters? only then build verification logic

longer term:
- [ ] don't write papers about code that doesn't exist
- [ ] build incrementally, validate frequently, publish one working module before the full system
- [ ] pair with someone who's done this before (data heavy + formal methods + graph models is three hard problems)

## reflection

the project is interesting but i was too ambitious on the timeline and too confident in the design. proposed a whole pipeline without actually building it. that's not engineering, that's fiction. 

the gap between "what i think the system should do" and "what i can demonstrate it does" is huge. that gap is where failure lives. next time the proposal reflects what's already working, not what i hope will work.

real talk: this could still be good work, but it needs to be rebuilt bottom-up. modules should work independently first. data should be clean and versioned before modeling. baselines should be established before fancy methods. and the formal stuff should be spec'd before it's promised.

going to spend the next week building Module A + B for one cell type and just getting that right. no more abstractions until i can show working code.